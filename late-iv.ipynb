{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd4b2a2d-6494-49d9-ad44-2301ffdd0ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "- late_lumpyITT_tests_simulations.csv\n",
      "- late_lumpyITT_tests_simulations_additional.csv\n",
      "\n",
      "=== Main table preview ===\n",
      "          Scenario    n  pC  pA           Alt  T2_mean    T2_sd  T2_reject_5pct  TJ_mean    TJ_sd  TJ_reject_5pct\n",
      "Null,strong,pC=0.3 2000 0.3 0.1          null 0.000583 0.000507           0.052 0.006407 0.005677           0.064\n",
      "  Null,weak,pC=0.1 2000 0.1 0.1          null 0.000769 0.000708           0.064 0.008560 0.007940           0.068\n",
      "Excl small (γ=0.2) 2000 0.3 0.1    excl_small 0.000733 0.000593           0.084 0.008009 0.006667           0.084\n",
      "  Excl big (γ=0.5) 2000 0.3 0.1      excl_big 0.006999 0.002107           1.000 0.077788 0.023616           1.000\n",
      "        Defiers 5% 2000 0.3 0.1 defiers_small 0.000877 0.000727           0.140 0.009674 0.008192           0.160\n",
      "       Defiers 10% 2000 0.3 0.1   defiers_big 0.001403 0.000948           0.360 0.015509 0.010574           0.380\n",
      "\n",
      "=== Additional table preview ===\n",
      "                  Scenario    n  pC  pA           Alt  T2_mean    T2_sd  T2_reject_5pct  TJ_mean    TJ_sd  TJ_reject_5pct\n",
      "Excl small (γ=0.2), n=1000 1000 0.3 0.1    excl_small 0.001038 0.001068           0.148 0.011296 0.011918           0.144\n",
      "Excl small (γ=0.2), n=4000 4000 0.3 0.1    excl_small 0.000671 0.000406           0.156 0.007357 0.004493           0.152\n",
      "        Defiers 5%, pC=0.1 2000 0.1 0.1 defiers_small 0.001012 0.000802           0.132 0.011255 0.009002           0.128\n",
      "        Defiers 5%, pC=0.5 2000 0.5 0.1 defiers_small 0.000562 0.000476           0.064 0.006130 0.005383           0.080\n"
     ]
    }
   ],
   "source": [
    "# Replication script for \"LATE: Only Compliers Move\" tests\n",
    "# Jupyter-friendly, no CLI. Run cells top-to-bottom.\n",
    "#\n",
    "# This script reproduces the simulation tables in the paper:\n",
    "#  - Test A: Uniform CDF identity (Cramér–von Mises T2)\n",
    "#  - Test B: GMM-style moment test over indicator basis (identity weighting by default)\n",
    "#\n",
    "# It also saves CSVs\n",
    "#  - late_lumpyITT_tests_simulations.csv\n",
    "#  - late_lumpyITT_tests_simulations_additional.csv\n",
    "#\n",
    "# Notes:\n",
    "#  - This block implements the RCT (no-covariates) design used for the main tables.\n",
    "#  - CDF validity is enforced using isotonic regression and clipping to [0,1].\n",
    "#  - Critical values are computed from null Monte Carlo and then used to assess size/power.\n",
    "#  - For a robust covariance GMM J-test, see the optional bootstrap W estimator stub below.\n",
    "\n",
    "#%pip install --upgrade --force-reinstall numpy pandas\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "# -----------------------\n",
    "# Reproducibility\n",
    "# -----------------------\n",
    "GLOBAL_SEED = 20250910\n",
    "rng_global = np.random.default_rng(GLOBAL_SEED)\n",
    "\n",
    "# -----------------------\n",
    "# DGP specification (RCT)\n",
    "# -----------------------\n",
    "@dataclass\n",
    "class DGPSpec:\n",
    "    n: int\n",
    "    pC: float\n",
    "    pA: float\n",
    "    tau_mean: float = 0.5\n",
    "    tau_sd: float = 0.4\n",
    "    alt: str = \"null\"    # \"null\", \"excl_small\", \"excl_big\", \"defiers_small\", \"defiers_big\", \"excl_all_small\"\n",
    "    gamma: float = 0.0   # magnitude for exclusion violation (direct effect)\n",
    "    pD: float = 0.0      # defier share if needed\n",
    "\n",
    "def simulate_dataset(spec: DGPSpec, rng: Optional[np.random.Generator]=None) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Principal-strata simulation with RCT assignment.\n",
    "    Types: C (compliers), A (always), N (never), D (defiers, only in alts).\n",
    "    Exclusion violation (excl_*): add gamma*Z to Y for noncompliers (or all types in excl_all_*).\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = rng_global\n",
    "    \n",
    "    n = spec.n\n",
    "    pC, pA = spec.pC, spec.pA\n",
    "    pN = max(1.0 - pC - pA - spec.pD, 0.0)\n",
    "    pD = spec.pD\n",
    "\n",
    "    # Draw types\n",
    "    probs = [pC, pA, pN, pD] if pD > 0 else [pC, pA, pN, 0.0]\n",
    "    types = rng.choice(['C','A','N','D'], size=n, p=probs)\n",
    "\n",
    "    # Assignment\n",
    "    Z = rng.binomial(1, 0.5, size=n)\n",
    "\n",
    "    # Potential treatment by type (monotonic unless defiers present)\n",
    "    D0 = np.zeros(n)\n",
    "    D1 = np.zeros(n)\n",
    "    D0[types=='C'] = 0; D1[types=='C'] = 1\n",
    "    D0[types=='A'] = 1; D1[types=='A'] = 1\n",
    "    D0[types=='N'] = 0; D1[types=='N'] = 0\n",
    "    D0[types=='D'] = 1; D1[types=='D'] = 0  # defiers violate monotonicity\n",
    "\n",
    "    # Heterogeneous complier effects\n",
    "    tau = rng.normal(spec.tau_mean, spec.tau_sd, size=n)\n",
    "    # Baseline potential outcome\n",
    "    Y0 = rng.normal(0.0, 1.0, size=n)\n",
    "    Y1 = Y0 + tau\n",
    "\n",
    "    # Observed treatment and outcome via exclusion (base)\n",
    "    D = np.where(Z==1, D1, D0)\n",
    "    Y = np.where(D==1, Y1, Y0)\n",
    "\n",
    "    # Exclusion violation variants\n",
    "    if spec.alt.startswith(\"excl\"):\n",
    "        if spec.alt == \"excl_all_small\":\n",
    "            Y = Y + spec.gamma * Z\n",
    "        else:\n",
    "            # direct effect for noncompliers\n",
    "            nonC_mask = (types!='C')\n",
    "            Y = Y + spec.gamma * Z * nonC_mask.astype(float)\n",
    "\n",
    "    return {\"Z\": Z.astype(int), \"D\": D.astype(int), \"Y\": Y.astype(float)}\n",
    "\n",
    "# -----------------------\n",
    "# CDF utilities\n",
    "# -----------------------\n",
    "def isotonic_cdf_enforce(grid: np.ndarray, values: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Enforce monotonicity and [0,1] range for a function measured on a sorted grid\n",
    "    using isotonic regression, then clip to [0,1].\n",
    "    \"\"\"\n",
    "    ir = IsotonicRegression(increasing=True, out_of_bounds=\"clip\")\n",
    "    fitted = ir.fit_transform(grid, values)\n",
    "    return np.clip(fitted, 0.0, 1.0)\n",
    "\n",
    "def compute_weighted_cdfs_rct(Z: np.ndarray, D: np.ndarray, Y: np.ndarray,\n",
    "                              grid: Optional[np.ndarray]=None) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute empirical CDFs FY1, FY0 and Abadie-weighted complier CDFs F1C, F0C\n",
    "    in an RCT without covariates. Enforce CDF validity via isotonic regression.\n",
    "    \"\"\"\n",
    "    n = len(Y)\n",
    "    # Grid\n",
    "    if grid is None:\n",
    "        qs = np.linspace(0.01, 0.99, 99)\n",
    "        grid = np.quantile(Y, qs)\n",
    "    grid = np.asarray(grid, dtype=float)\n",
    "    # First-stage\n",
    "    e = 0.5\n",
    "    p1 = D[Z==1].mean() if (Z==1).any() else 0.0\n",
    "    p0 = D[Z==0].mean() if (Z==0).any() else 0.0\n",
    "    pC_hat = p1 - p0\n",
    "    if pC_hat <= 1e-10:\n",
    "        pC_hat = 1e-10\n",
    "\n",
    "    # Empirical CDFs\n",
    "    FY1 = np.array([ (Y[Z==1] <= t).mean() if (Z==1).any() else 0.0 for t in grid ])\n",
    "    FY0 = np.array([ (Y[Z==0] <= t).mean() if (Z==0).any() else 0.0 for t in grid ])\n",
    "    FY1 = isotonic_cdf_enforce(grid, FY1)\n",
    "    FY0 = isotonic_cdf_enforce(grid, FY0)\n",
    "\n",
    "    # Weights for complier marginals\n",
    "    w1 = (Z/e) * (D - p0)            # targets Y(1)|C\n",
    "    w0 = ((1-Z)/(1-e)) * (p1 - D)    # targets Y(0)|C\n",
    "\n",
    "    # Weighted CDFs for compliers\n",
    "    num1 = np.array([ ((Y<=t).astype(float) * w1).sum() for t in grid ])\n",
    "    num0 = np.array([ ((Y<=t).astype(float) * w0).sum() for t in grid ])\n",
    "    F1C = num1 / (pC_hat * n)\n",
    "    F0C = num0 / (pC_hat * n)\n",
    "\n",
    "    # Enforce CDF validity\n",
    "    F1C = isotonic_cdf_enforce(grid, F1C)\n",
    "    F0C = isotonic_cdf_enforce(grid, F0C)\n",
    "\n",
    "    # Identity difference\n",
    "    diff = (FY1 - FY0) - pC_hat*(F1C - F0C)\n",
    "\n",
    "    return {\n",
    "        \"grid\": grid, \"FY1\": FY1, \"FY0\": FY0,\n",
    "        \"F1C\": F1C, \"F0C\": F0C, \"diff\": diff, \"pC_hat\": pC_hat\n",
    "    }\n",
    "\n",
    "# -----------------------\n",
    "# Test statistics\n",
    "# -----------------------\n",
    "def stat_T2(diff: np.ndarray, grid: Optional[np.ndarray]=None) -> float:\n",
    "    \"\"\"\n",
    "    Cramér–von Mises style integrated squared error over the grid.\n",
    "    Uses an unweighted mean across grid points.\n",
    "    \"\"\"\n",
    "    return float(np.mean(diff**2))\n",
    "\n",
    "def stat_GMM_identity(Y: np.ndarray, Z: np.ndarray, stats: Dict[str,np.ndarray], J: int=10) -> float:\n",
    "    \"\"\"\n",
    "    Moment vector m at J indicator cutpoints with identity weighting:\n",
    "    m_j = [FY1(t_j) - FY0(t_j)] - pC_hat * [F1C(t_j) - F0C(t_j)]\n",
    "    Returns ||m||^2. This matches the simple GMM used in the main replication.\n",
    "    \"\"\"\n",
    "    qs = np.linspace(0.1, 0.9, J)\n",
    "    tjs = np.quantile(Y, qs)\n",
    "    grid = stats[\"grid\"]\n",
    "    # nearest indices on grid\n",
    "    idx = np.searchsorted(grid, tjs, side=\"left\")\n",
    "    idx = np.clip(idx, 0, len(grid)-1)\n",
    "    # empirical CDFs at t_j\n",
    "    FY1_t = np.array([ (Y[Z==1] <= t).mean() for t in tjs ])\n",
    "    FY0_t = np.array([ (Y[Z==0] <= t).mean() for t in tjs ])\n",
    "    m = (FY1_t - FY0_t) - stats[\"pC_hat\"] * (stats[\"F1C\"][idx] - stats[\"F0C\"][idx])\n",
    "    return float(np.dot(m, m))\n",
    "\n",
    "# -----------------------\n",
    "# Simulation runner\n",
    "# -----------------------\n",
    "def run_null_draws(spec: DGPSpec, R: int=300, n_grid: int=101, rng: Optional[np.random.Generator]=None):\n",
    "    T2_vals, TJ_vals = [], []\n",
    "    if rng is None:\n",
    "        rng = rng_global\n",
    "    for r in range(R):\n",
    "        dat = simulate_dataset(spec, rng)\n",
    "        Z, D, Y = dat[\"Z\"], dat[\"D\"], dat[\"Y\"]\n",
    "        grid = np.quantile(Y, np.linspace(0.01,0.99,n_grid-2))\n",
    "        stats = compute_weighted_cdfs_rct(Z,D,Y,grid=grid)\n",
    "        T2_vals.append(stat_T2(stats[\"diff\"], grid))\n",
    "        TJ_vals.append(stat_GMM_identity(Y,Z,stats,J=10))\n",
    "    return np.array(T2_vals), np.array(TJ_vals)\n",
    "\n",
    "def run_eval_draws(spec: DGPSpec, crit_T2: float, crit_TJ: float,\n",
    "                   R: int=250, n_grid: int=101, rng: Optional[np.random.Generator]=None):\n",
    "    T2_vals, TJ_vals = [], []\n",
    "    rejs_T2, rejs_TJ = 0, 0\n",
    "    if rng is None:\n",
    "        rng = rng_global\n",
    "    for r in range(R):\n",
    "        dat = simulate_dataset(spec, rng)\n",
    "        Z, D, Y = dat[\"Z\"], dat[\"D\"], dat[\"Y\"]\n",
    "        grid = np.quantile(Y, np.linspace(0.01,0.99,n_grid-2))\n",
    "        stats = compute_weighted_cdfs_rct(Z,D,Y,grid=grid)\n",
    "        T2 = stat_T2(stats[\"diff\"], grid)\n",
    "        TJ = stat_GMM_identity(Y,Z,stats,J=10)\n",
    "        T2_vals.append(T2); TJ_vals.append(TJ)\n",
    "        rejs_T2 += int(T2 > crit_T2)\n",
    "        rejs_TJ += int(TJ > crit_TJ)\n",
    "    out = {\n",
    "        \"T2_mean\": float(np.mean(T2_vals)),\n",
    "        \"T2_sd\": float(np.std(T2_vals)),\n",
    "        \"T2_reject_5pct\": float(rejs_T2 / R),\n",
    "        \"TJ_mean\": float(np.mean(TJ_vals)),\n",
    "        \"TJ_sd\": float(np.std(TJ_vals)),\n",
    "        \"TJ_reject_5pct\": float(rejs_TJ / R),\n",
    "    }\n",
    "    return out\n",
    "\n",
    "def replicate_main_tables() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Reproduce the main and additional tables from the paper.\n",
    "    Returns two dataframes and saves them to CSV.\n",
    "    \"\"\"\n",
    "    # Scenarios\n",
    "    scenarios = [\n",
    "        (\"Null,strong,pC=0.3\", DGPSpec(n=2000, pC=0.30, pA=0.10, alt=\"null\")),\n",
    "        (\"Null,weak,pC=0.1\",   DGPSpec(n=2000, pC=0.10, pA=0.10, alt=\"null\")),\n",
    "        (\"Excl small (γ=0.2)\", DGPSpec(n=2000, pC=0.30, pA=0.10, alt=\"excl_small\", gamma=0.2)),\n",
    "        (\"Excl big (γ=0.5)\",   DGPSpec(n=2000, pC=0.30, pA=0.10, alt=\"excl_big\", gamma=0.5)),\n",
    "        (\"Defiers 5%\",         DGPSpec(n=2000, pC=0.30, pA=0.10, alt=\"defiers_small\", pD=0.05)),\n",
    "        (\"Defiers 10%\",        DGPSpec(n=2000, pC=0.30, pA=0.10, alt=\"defiers_big\", pD=0.10)),\n",
    "    ]\n",
    "    # Null criticals\n",
    "    T2_null_strong, TJ_null_strong = run_null_draws(scenarios[0][1], R=400)\n",
    "    T2_null_weak,   TJ_null_weak   = run_null_draws(scenarios[1][1], R=400)\n",
    "    crit_T2_strong = float(np.quantile(T2_null_strong, 0.95))\n",
    "    crit_TJ_strong = float(np.quantile(TJ_null_strong, 0.95))\n",
    "    crit_T2_weak   = float(np.quantile(T2_null_weak,   0.95))\n",
    "    crit_TJ_weak   = float(np.quantile(TJ_null_weak,   0.95))\n",
    "\n",
    "    # Evaluate scenarios\n",
    "    rows = []\n",
    "    for name, spec in scenarios:\n",
    "        if \"weak\" in name:\n",
    "            crit_T2, crit_TJ = crit_T2_weak, crit_TJ_weak\n",
    "        else:\n",
    "            crit_T2, crit_TJ = crit_T2_strong, crit_TJ_strong\n",
    "        res = run_eval_draws(spec, crit_T2, crit_TJ, R=250)\n",
    "        rows.append({\n",
    "            \"Scenario\": name,\n",
    "            \"n\": spec.n, \"pC\": spec.pC, \"pA\": spec.pA, \"Alt\": spec.alt,\n",
    "            **res\n",
    "        })\n",
    "    df_main = pd.DataFrame(rows)\n",
    "    csv_main = \"late_lumpyITT_tests_simulations.csv\"\n",
    "    df_main.to_csv(csv_main, index=False)\n",
    "\n",
    "    # Additional sensitivity scenarios\n",
    "    more = [\n",
    "        (\"Excl small (γ=0.2), n=1000\", DGPSpec(n=1000, pC=0.30, pA=0.10, alt=\"excl_small\", gamma=0.2)),\n",
    "        (\"Excl small (γ=0.2), n=4000\", DGPSpec(n=4000, pC=0.30, pA=0.10, alt=\"excl_small\", gamma=0.2)),\n",
    "        (\"Defiers 5%, pC=0.1\",         DGPSpec(n=2000, pC=0.10, pA=0.10, alt=\"defiers_small\", pD=0.05)),\n",
    "        (\"Defiers 5%, pC=0.5\",         DGPSpec(n=2000, pC=0.50, pA=0.10, alt=\"defiers_small\", pD=0.05)),\n",
    "    ]\n",
    "    rows2 = []\n",
    "    for name, spec in more:\n",
    "        # compute matching null criticals for each (n,pC)\n",
    "        T2n, TJn = run_null_draws(DGPSpec(n=spec.n, pC=spec.pC, pA=spec.pA, alt=\"null\"), R=300)\n",
    "        crit_T2, crit_TJ = float(np.quantile(T2n,0.95)), float(np.quantile(TJn,0.95))\n",
    "        res = run_eval_draws(spec, crit_T2, crit_TJ, R=250)\n",
    "        rows2.append({\n",
    "            \"Scenario\": name,\n",
    "            \"n\": spec.n, \"pC\": spec.pC, \"pA\": spec.pA, \"Alt\": spec.alt,\n",
    "            **res\n",
    "        })\n",
    "    df_more = pd.DataFrame(rows2)\n",
    "    csv_more = \"late_lumpyITT_tests_simulations_additional.csv\"\n",
    "    df_more.to_csv(csv_more, index=False)\n",
    "\n",
    "    print(\"Saved:\")\n",
    "    print(f\"- {csv_main}\")\n",
    "    print(f\"- {csv_more}\")\n",
    "    return df_main, df_more\n",
    "\n",
    "# -----------------------\n",
    "# Optional: robust covariance GMM (bootstrap W)\n",
    "# -----------------------\n",
    "def gmm_bootstrap_covariance(Y: np.ndarray, Z: np.ndarray, stats: Dict[str,np.ndarray],\n",
    "                             J: int=10, B: int=200, rng: Optional[np.random.Generator]=None) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Optional helper to estimate a covariance matrix for the GMM moment vector via a simple nonparametric bootstrap.\n",
    "    Returns (m_vector, W_hat, T_J) where T_J = n m' W^{-1} m.\n",
    "    This adds runtime. Not used in main replication tables.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = rng_global\n",
    "    qs = np.linspace(0.1, 0.9, J)\n",
    "    tjs = np.quantile(Y, qs)\n",
    "    grid = stats[\"grid\"]\n",
    "    idx = np.searchsorted(grid, tjs, side=\"left\")\n",
    "    idx = np.clip(idx, 0, len(grid)-1)\n",
    "    FY1_t = np.array([ (Y[Z==1] <= t).mean() for t in tjs ])\n",
    "    FY0_t = np.array([ (Y[Z==0] <= t).mean() for t in tjs ])\n",
    "    m = (FY1_t - FY0_t) - stats[\"pC_hat\"] * (stats[\"F1C\"][idx] - stats[\"F0C\"][idx])\n",
    "    # bootstrap covariance\n",
    "    n = len(Y)\n",
    "    Ms = []\n",
    "    for b in range(B):\n",
    "        idx_b = rng.integers(0, n, size=n)\n",
    "        Yb, Zb = Y[idx_b], Z[idx_b]\n",
    "        FY1_t_b = np.array([ (Yb[Zb==1] <= t).mean() for t in tjs ])\n",
    "        FY0_t_b = np.array([ (Yb[Zb==0] <= t).mean() for t in tjs ])\n",
    "        # reuse complier CDFs from full sample for speed; for more accuracy recompute stats on bootstrap sample\n",
    "        mb = (FY1_t_b - FY0_t_b) - stats[\"pC_hat\"] * (stats[\"F1C\"][idx] - stats[\"F0C\"][idx])\n",
    "        Ms.append(mb)\n",
    "    Ms = np.vstack(Ms)\n",
    "    W = np.cov(Ms.T, bias=False)\n",
    "    # Regularize if needed\n",
    "    eps = 1e-10\n",
    "    W += eps * np.eye(J)\n",
    "    TJ = float(n * m.T @ np.linalg.inv(W) @ m)\n",
    "    return m, W, TJ\n",
    "\n",
    "# -----------------------\n",
    "# Run a quick smoke test (lightweight) and then run the full replication\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Smoke test with fewer reps (adjust as needed)\n",
    "    df_main, df_more = replicate_main_tables()\n",
    "    print(\"\\n=== Main table preview ===\")\n",
    "    print(df_main.to_string(index=False))\n",
    "    print(\"\\n=== Additional table preview ===\")\n",
    "    print(df_more.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cb12df-44c4-445f-97a6-afa076ce2c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
